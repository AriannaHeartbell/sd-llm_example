{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AriannaHeartbell/sd-llm_example/blob/main/upscaler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "datasetmaker from https://github.com/hollowstrawberry/kohya-colab/blob/main/Dataset_Maker.ipynb"
      ],
      "metadata": {
        "id": "pSvijVO_o2E0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spandrel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl_60sz1xi0g",
        "outputId": "731576c6-610b-41eb-846e-4f2165d07c62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spandrel\n",
            "  Downloading spandrel-0.3.4-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from spandrel) (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from spandrel) (0.18.1+cu121)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from spandrel) (0.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from spandrel) (1.26.4)\n",
            "Collecting einops (from spandrel)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from spandrel) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->spandrel) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->spandrel) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->spandrel) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->spandrel) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->spandrel) (2024.6.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->spandrel)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->spandrel)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->spandrel)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->spandrel)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->spandrel)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->spandrel)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->spandrel)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->spandrel)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->spandrel)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->spandrel)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->spandrel)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->spandrel) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->spandrel)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->spandrel) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->spandrel) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->spandrel) (1.3.0)\n",
            "Downloading spandrel-0.3.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m269.0/269.0 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, einops, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, spandrel\n",
            "Successfully installed einops-0.8.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 spandrel-0.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtxlTebIxmaF",
        "outputId": "79f0983c-365a-499c-fd7b-3bfde77bf622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì°¸ì¡°: https://chat.openai.com/share/ab2673a1-8a17-4551-a317-eeda61ec3654"
      ],
      "metadata": {
        "id": "_QN9KhiJ2j4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spandrel import ImageModelDescriptor, ModelLoader\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.transforms import Resize\n",
        "import os\n",
        "\n",
        "\n",
        "# í´ë” ê²½ë¡œ ë³€ìˆ˜ ì„¤ì •\n",
        "input_folder = \"/content/drive/MyDrive/Loras/anime_himeno/1\"  #@param {type:\"string\"}\n",
        "output_folder = \"/content/drive/MyDrive/Loras/anime_himeno/2\"  #@param {type:\"string\"}\n",
        "model_choice = \"4x-AnimeSharp\" #@param [\"4x-AnimeSharp\", \"4x-Anime6B\"]\n",
        "\n",
        "# ëª¨ë¸ì— ë”°ë¼ ê²½ë¡œë¥¼ ì •í•´ì£¼ëŠ” ì½”ë“œì•¼!\n",
        "if model_choice == \"4x-AnimeSharp\":\n",
        "    model_path = \"/content/drive/MyDrive/SD/models/ESRGAN/4x-AnimeSharp1.pth\"\n",
        "elif model_choice == \"4x-Anime6B\":\n",
        "    model_path = \"/content/drive/MyDrive/SD/models/RealESRGAN/RealESRGAN_x4plus_anime_6B.pth\"\n",
        "else:\n",
        "    raise ValueError(\"Invalid model choice\")\n",
        "\n",
        "# ëª¨ë¸ ë¡œë“œ\n",
        "model = ModelLoader().load_from_file(model_path)\n",
        "assert isinstance(model, ImageModelDescriptor)\n",
        "model.cuda().eval()\n",
        "\n",
        "# ì´ë¯¸ì§€ ì²˜ë¦¬ í•¨ìˆ˜\n",
        "def process(image: torch.Tensor) -> torch.Tensor:\n",
        "    with torch.no_grad():\n",
        "        return model(image)\n",
        "\n",
        "# ì´ë¯¸ì§€ í¬ê¸° ì¡°ì ˆ í•¨ìˆ˜\n",
        "def resize_image(image, scale_factor=0.5):\n",
        "    # ì´ë¯¸ì§€ë¥¼ RGBAë¡œ ë³€í™˜ (íˆ¬ëª…ë„ê°€ ìˆëŠ” íŒ”ë ˆíŠ¸ ì´ë¯¸ì§€ì¸ ê²½ìš°)\n",
        "    if image.mode == 'P':\n",
        "        image = image.convert('RGBA')\n",
        "    elif image.mode != 'RGB':\n",
        "        image = image.convert('RGB')\n",
        "\n",
        "    width, height = image.size\n",
        "    new_width, new_height = int(width * scale_factor), int(height * scale_factor)\n",
        "    resize = Resize((new_height, new_width))\n",
        "    return resize(image)\n",
        "\n",
        "# í•˜ìœ„ í´ë”ë¥¼ í¬í•¨í•œ ëª¨ë“  ì´ë¯¸ì§€ íŒŒì¼ì˜ ê²½ë¡œ ì°¾ê¸°\n",
        "image_paths = []\n",
        "for subdir, dirs, files in os.walk(input_folder):\n",
        "    for file in files:\n",
        "        filepath = os.path.join(subdir, file)\n",
        "        if filepath.endswith(\".jpg\") or filepath.endswith(\".png\"):\n",
        "            image_paths.append(filepath)\n",
        "\n",
        "# ê° ì´ë¯¸ì§€ ì²˜ë¦¬\n",
        "for image_path in image_paths:\n",
        "    image = Image.open(image_path)\n",
        "    resized_image = resize_image(image)  # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì ˆ\n",
        "    image_tensor = TF.to_tensor(resized_image).unsqueeze(0).cuda()  # ë³€í™˜ëœ ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜í•˜ê³  GPUë¡œ ë³´ëƒ„\n",
        "    processed_tensor = process(image_tensor)  # ì´ë¯¸ì§€ ì²˜ë¦¬\n",
        "    processed_image = TF.to_pil_image(processed_tensor.squeeze(0))  # í…ì„œë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜\n",
        "\n",
        "    # ìƒëŒ€ ê²½ë¡œë¥¼ ì´ìš©í•´ ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
        "    relative_path = os.path.relpath(image_path, input_folder)\n",
        "    save_path = os.path.join(output_folder, relative_path)\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)  # í•„ìš”í•œ ê²½ìš° í´ë” ìƒì„±\n",
        "    processed_image.save(save_path)\n"
      ],
      "metadata": {
        "id": "o7_5IQeB0ibJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì•ŒíŒŒì±„ë„ ì œê±° ê´€ë ¨"
      ],
      "metadata": {
        "id": "U-Brz32B6gFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# ì´ë¯¸ì§€ ì—´ê¸°\n",
        "img = Image.open('/content/drive/MyDrive/SD/UP/c/hbr_yuina_collabo.png')\n",
        "\n",
        "# RGBë¡œ ë³€í™˜\n",
        "rgb_img = img.convert('RGB')\n",
        "\n",
        "# ì´ë¯¸ì§€ ì €ì¥\n",
        "rgb_img.save('/content/drive/MyDrive/SD/UP/c/your_image_no_alpha_alphadele.png')\n"
      ],
      "metadata": {
        "id": "XNscg1Zr6EWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TqMLDot1xgs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "COLAB = True\n",
        "\n",
        "if COLAB:\n",
        "  from google.colab.output import clear as clear_output\n",
        "else:\n",
        "  from IPython.display import clear_output\n",
        "\n",
        "#@title ## ğŸš© Start Here\n",
        "\n",
        "#@markdown ### 1ï¸âƒ£ Setup\n",
        "#@markdown This cell will load some requirements and create the necessary folders in your Google Drive. <p>\n",
        "#@markdown Your project name can't contain spaces but it can contain a single / to make a subfolder in your dataset.\n",
        "project_name = \"\" #@param {type:\"string\"}\n",
        "project_name = project_name.strip()\n",
        "#@markdown The folder structure doesn't matter and is purely for comfort. Make sure to always pick the same one. I like organizing by project.\n",
        "folder_structure = \"Organize by project (MyDrive/Loras/project_name/dataset)\" #@param [\"Organize by category (MyDrive/lora_training/datasets/project_name)\", \"Organize by project (MyDrive/Loras/project_name/dataset)\"]\n",
        "\n",
        "if not project_name or any(c in project_name for c in \" .()\\\"'\\\\\") or project_name.count(\"/\") > 1:\n",
        "  print(\"Please write a valid project_name.\")\n",
        "else:\n",
        "  if COLAB and not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    print(\"ğŸ“‚ Connecting to Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "  project_base = project_name if \"/\" not in project_name else project_name[:project_name.rfind(\"/\")]\n",
        "  project_subfolder = project_name if \"/\" not in project_name else project_name[project_name.rfind(\"/\")+1:]\n",
        "\n",
        "  root_dir = \"/content\" if COLAB else \"~/Loras\"\n",
        "  deps_dir = os.path.join(root_dir, \"deps\")\n",
        "\n",
        "  if \"/Loras\" in folder_structure:\n",
        "    main_dir      = os.path.join(root_dir, \"drive/MyDrive/Loras\") if COLAB else root_dir\n",
        "    config_folder = os.path.join(main_dir, project_base)\n",
        "    images_folder = os.path.join(main_dir, project_base, \"dataset\")\n",
        "    if \"/\" in project_name:\n",
        "      images_folder = os.path.join(images_folder, project_subfolder)\n",
        "  else:\n",
        "    main_dir      = os.path.join(root_dir, \"drive/MyDrive/lora_training\") if COLAB else root_dir\n",
        "    config_folder = os.path.join(main_dir, \"config\", project_name)\n",
        "    images_folder = os.path.join(main_dir, \"datasets\", project_name)\n",
        "\n",
        "  for dir in [main_dir, deps_dir, images_folder, config_folder]:\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "  print(f\"âœ… Project {project_name} is ready!\")\n",
        "  step1_installed_flag = True\n"
      ],
      "metadata": {
        "id": "kBrijtNlHkmC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c17f9011-7776-4447-a3c3-839726391f37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“‚ Connecting to Google Drive...\n",
            "Mounted at /content/drive\n",
            "âœ… Project fajyobore2_pony is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if \"step1_installed_flag\" not in globals():\n",
        "  raise Exception(\"Please run step 1 first!\")\n",
        "\n",
        "#@markdown ### 4ï¸âƒ£ Tag your images\n",
        "#@markdown We will be using AI to automatically tag your images, specifically [Waifu Diffusion](https://huggingface.co/SmilingWolf/wd-v1-4-swinv2-tagger-v2) in the case of anime and [BLIP](https://huggingface.co/spaces/Salesforce/BLIP) in the case of photos.\n",
        "#@markdown Giving tags/captions to your images allows for much better training. This process should take a couple minutes. <p>\n",
        "method = \"Anime tags\" #@param [\"Anime tags\", \"Photo captions\"]\n",
        "#@markdown **Anime:** The threshold is the minimum level of confidence the tagger must have in order to include a tag. Lower threshold = More tags. Recommended 0.35 to 0.5\n",
        "tag_threshold = 0.15 #@param {type:\"slider\", min:0.0, max:1.0, step:0.01}\n",
        "blacklist_tags = \"bangs, breasts, multicolored hair, two-tone hair, gradient hair, virtual youtuber, parody, style parody, official alternate costume, official alternate hairstyle, official alternate hair length, alternate costume, alternate hairstyle, alternate hair length, alternate hair color\" #@param {type:\"string\"}\n",
        "#@markdown **Photos:** The minimum and maximum length of tokens/words in each caption.\n",
        "caption_min = 10 #@param {type:\"number\"}\n",
        "caption_max = 75 #@param {type:\"number\"}\n",
        "\n",
        "%env PYTHONPATH=/env/python\n",
        "os.chdir(root_dir)\n",
        "kohya = \"/content/kohya-trainer\"\n",
        "if not os.path.exists(kohya):\n",
        "  !git clone https://github.com/kohya-ss/sd-scripts {kohya}\n",
        "  os.chdir(kohya)\n",
        "  !git reset --hard 9a67e0df390033a89f17e70df5131393692c2a55\n",
        "  os.chdir(root_dir)\n",
        "\n",
        "if \"tags\" in method:\n",
        "  if \"step4a_installed_flag\" not in globals():\n",
        "    print(\"\\nğŸ­ Installing dependencies...\\n\")\n",
        "    !pip install accelerate==0.15.0 diffusers[torch]==0.10.2 einops==0.6.0 tensorflow==2.15.0 keras==2.15.0 transformers safetensors huggingface-hub torchvision albumentations jax==0.4.23 jaxlib==0.4.23\n",
        "    if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "      clear_output()\n",
        "      step4a_installed_flag = True\n",
        "    else:\n",
        "      print(\"âŒ Error installing dependencies, trying to continue anyway...\")\n",
        "\n",
        "  print(\"\\nğŸš¶â€â™‚ï¸ Launching program...\\n\")\n",
        "\n",
        "  os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "  %env PYTHONPATH={kohya}\n",
        "  !python {kohya}/finetune/tag_images_by_wd14_tagger.py \\\n",
        "    {images_folder} \\\n",
        "    --repo_id=SmilingWolf/wd-v1-4-swinv2-tagger-v2 \\\n",
        "    --model_dir={root_dir} \\\n",
        "    --thresh={tag_threshold} \\\n",
        "    --batch_size=8 \\\n",
        "    --caption_extension=.txt \\\n",
        "    --force_download\n",
        "\n",
        "  if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "    print(\"removing underscores and blacklist...\")\n",
        "    blacklisted_tags = [t.strip() for t in blacklist_tags.split(\",\")]\n",
        "    from collections import Counter\n",
        "    top_tags = Counter()\n",
        "    for txt in [f for f in os.listdir(images_folder) if f.lower().endswith(\".txt\")]:\n",
        "      with open(os.path.join(images_folder, txt), 'r') as f:\n",
        "        tags = [t.strip() for t in f.read().split(\",\")]\n",
        "        tags = [t.replace(\"_\", \" \") if len(t) > 3 else t for t in tags]\n",
        "        tags = [t for t in tags if t not in blacklisted_tags]\n",
        "      top_tags.update(tags)\n",
        "      with open(os.path.join(images_folder, txt), 'w') as f:\n",
        "        f.write(\", \".join(tags))\n",
        "\n",
        "    %env PYTHONPATH=/env/python\n",
        "    clear_output()\n",
        "    print(f\"ğŸ“Š Tagging complete. Here are the top 50 tags in your dataset:\")\n",
        "    print(\"\\n\".join(f\"{k} ({v})\" for k, v in top_tags.most_common(50)))\n",
        "\n",
        "\n",
        "else: # Photos\n",
        "  if \"step4b_installed_flag\" not in globals():\n",
        "    print(\"\\nğŸ­ Installing dependencies...\\n\")\n",
        "    !pip install timm==0.6.12 fairscale==0.4.13 transformers==4.26.0 requests==2.28.2 accelerate==0.15.0 diffusers[torch]==0.10.2 einops==0.6.0 safetensors==0.2.6 jax==0.4.23 jaxlib==0.4.23\n",
        "    if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "      clear_output()\n",
        "      step4b_installed_flag = True\n",
        "    else:\n",
        "      print(\"âŒ Error installing dependencies, trying to continue anyway...\")\n",
        "\n",
        "  print(\"\\nğŸš¶â€â™‚ï¸ Launching program...\\n\")\n",
        "\n",
        "  os.chdir(kohya)\n",
        "  %env PYTHONPATH={kohya}\n",
        "  !python {kohya}/finetune/make_captions.py \\\n",
        "    {images_folder} \\\n",
        "    --beam_search \\\n",
        "    --max_data_loader_n_workers=2 \\\n",
        "    --batch_size=8 \\\n",
        "    --min_length={caption_min} \\\n",
        "    --max_length={caption_max} \\\n",
        "    --caption_extension=.txt\n",
        "\n",
        "  if not get_ipython().__dict__['user_ns']['_exit_code']:\n",
        "    import random\n",
        "    captions = [f for f in os.listdir(images_folder) if f.lower().endswith(\".txt\")]\n",
        "    sample = []\n",
        "    for txt in random.sample(captions, min(10, len(captions))):\n",
        "      with open(os.path.join(images_folder, txt), 'r') as f:\n",
        "        sample.append(f.read())\n",
        "\n",
        "    os.chdir(root_dir)\n",
        "    %env PYTHONPATH=/env/python\n",
        "    clear_output()\n",
        "    print(f\"ğŸ“Š Captioning complete. Here are {len(sample)} example captions from your dataset:\")\n",
        "    print(\"\".join(sample))\n"
      ],
      "metadata": {
        "id": "3HzSJuWxHmtp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f42466d4-0c58-45b4-b422-266ddf65ee13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTHONPATH=/env/python\n",
            "\n",
            "ğŸš¶â€â™‚ï¸ Launching program...\n",
            "\n",
            "env: PYTHONPATH=/content/kohya-trainer\n",
            "2024-08-01 15:50:57.944759: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-01 15:50:57.944833: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-01 15:50:57.947208: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
            "downloading wd14 tagger model from hf_hub. id: SmilingWolf/wd-v1-4-swinv2-tagger-v2\n",
            "keras_metadata.pb: 100% 448k/448k [00:00<00:00, 18.0MB/s]\n",
            "saved_model.pb: 100% 37.6M/37.6M [00:00<00:00, 130MB/s]\n",
            "selected_tags.csv: 100% 254k/254k [00:00<00:00, 9.14MB/s]\n",
            "variables.data-00000-of-00001: 100% 385M/385M [00:01<00:00, 238MB/s]\n",
            "variables.index: 100% 24.2k/24.2k [00:00<00:00, 97.7MB/s]\n",
            "INFO:absl:Fingerprint not found. Saved model loading will continue.\n",
            "INFO:absl:path_and_singleprint metric could not be logged. Saved model loading will continue.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "found 93 images.\n",
            " 16% 15/93 [00:14<01:17,  1.00it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if \"step1_installed_flag\" not in globals():\n",
        "  raise Exception(\"Please run step 1 first!\")\n",
        "\n",
        "#@markdown ### 5ï¸âƒ£ Curate your tags\n",
        "#@markdown Modify your dataset's tags. You can run this cell multiple times with different parameters. <p>\n",
        "\n",
        "#@markdown Put an activation tag at the start of every text file. This is useful to make learning better and activate your Lora easier. Set `keep_tokens` to 1 when training.<p>\n",
        "#@markdown Common tags that are removed such as hair color, etc. will be \"absorbed\" by your activation tag.\n",
        "global_activation_tag = \"hatsune miku\" #@param {type:\"string\"}\n",
        "remove_tags = \"\" #@param {type:\"string\"}\n",
        "#@markdown &nbsp;\n",
        "\n",
        "#@markdown In this advanced section, you can search text files containing matching tags, and replace them with less/more/different tags. If you select the checkbox below, any extra tags will be put at the start of the file, letting you assign different activation tags to different parts of your dataset. Still, you may want a more advanced tool for this.\n",
        "search_tags = \"\" #@param {type:\"string\"}\n",
        "replace_with = \"\" #@param {type:\"string\"}\n",
        "search_mode = \"OR\" #@param [\"OR\", \"AND\"]\n",
        "new_becomes_activation_tag = False #@param {type:\"boolean\"}\n",
        "#@markdown These may be useful sometimes. Will remove existing activation tags, be careful.\n",
        "sort_alphabetically = False #@param {type:\"boolean\"}\n",
        "remove_duplicates = False #@param {type:\"boolean\"}\n",
        "\n",
        "def split_tags(tagstr):\n",
        "  return [s.strip() for s in tagstr.split(\",\") if s.strip()]\n",
        "\n",
        "activation_tag_list = split_tags(global_activation_tag)\n",
        "remove_tags_list = split_tags(remove_tags)\n",
        "search_tags_list = split_tags(search_tags)\n",
        "replace_with_list = split_tags(replace_with)\n",
        "replace_new_list = [t for t in replace_with_list if t not in search_tags_list]\n",
        "\n",
        "replace_with_list = [t for t in replace_with_list if t not in replace_new_list]\n",
        "replace_new_list.reverse()\n",
        "activation_tag_list.reverse()\n",
        "\n",
        "remove_count = 0\n",
        "replace_count = 0\n",
        "\n",
        "for txt in [f for f in os.listdir(images_folder) if f.lower().endswith(\".txt\")]:\n",
        "\n",
        "  with open(os.path.join(images_folder, txt), 'r') as f:\n",
        "    tags = [s.strip() for s in f.read().split(\",\")]\n",
        "\n",
        "  if remove_duplicates:\n",
        "    tags = list(set(tags))\n",
        "  if sort_alphabetically:\n",
        "    tags.sort()\n",
        "\n",
        "  for rem in remove_tags_list:\n",
        "    if rem in tags:\n",
        "      remove_count += 1\n",
        "      tags.remove(rem)\n",
        "\n",
        "  if \"AND\" in search_mode and all(r in tags for r in search_tags_list) \\\n",
        "      or \"OR\" in search_mode and any(r in tags for r in search_tags_list):\n",
        "    replace_count += 1\n",
        "    for rem in search_tags_list:\n",
        "      if rem in tags:\n",
        "        tags.remove(rem)\n",
        "    for add in replace_with_list:\n",
        "      if add not in tags:\n",
        "        tags.append(add)\n",
        "    for new in replace_new_list:\n",
        "      if new_becomes_activation_tag:\n",
        "        if new in tags:\n",
        "          tags.remove(new)\n",
        "        tags.insert(0, new)\n",
        "      else:\n",
        "        if new not in tags:\n",
        "          tags.append(new)\n",
        "\n",
        "  for act in activation_tag_list:\n",
        "    if act in tags:\n",
        "      tags.remove(act)\n",
        "    tags.insert(0, act)\n",
        "\n",
        "  with open(os.path.join(images_folder, txt), 'w') as f:\n",
        "    f.write(\", \".join(tags))\n",
        "\n",
        "if global_activation_tag:\n",
        "  print(f\"\\nğŸ“ Applied new activation tag(s): {', '.join(activation_tag_list)}\")\n",
        "if remove_tags:\n",
        "  print(f\"\\nğŸš® Removed {remove_count} tags.\")\n",
        "if search_tags:\n",
        "  print(f\"\\nğŸ’« Replaced in {replace_count} files.\")\n",
        "print(\"\\nâœ… Done! Check your updated tags in the Extras below.\")\n"
      ],
      "metadata": {
        "id": "450NpFpSHpl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"step1_installed_flag\" not in globals():\n",
        "  raise Exception(\"Please run step 1 first!\")\n",
        "\n",
        "#@markdown ### ğŸ“ˆ Analyze Tags\n",
        "#@markdown Perhaps you need another look at your dataset.\n",
        "show_top_tags = 50 #@param {type:\"number\"}\n",
        "\n",
        "from collections import Counter\n",
        "top_tags = Counter()\n",
        "\n",
        "for txt in [f for f in os.listdir(images_folder) if f.lower().endswith(\".txt\")]:\n",
        "  with open(os.path.join(images_folder, txt), 'r') as f:\n",
        "    top_tags.update([s.strip() for s in f.read().split(\",\")])\n",
        "\n",
        "top_tags = Counter(top_tags)\n",
        "print(f\"ğŸ“Š Top {show_top_tags} tags:\")\n",
        "for k, v in top_tags.most_common(show_top_tags):\n",
        "  print(f\"{k} ({v})\")"
      ],
      "metadata": {
        "id": "DYySiZ0yV0-Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}